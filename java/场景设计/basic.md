[TOC]
《大型网站系统与Java中间件实践》
# 银行转账问题
从a账户转到b账户的设计
- a的钱不够，直接判断不够，就算了
- 如果有很多人都在给b转，那么a可以通过自旋转账
- a减完钱，b还没加上去，就挂了，这里考虑用事务

## springboot事务控制
使用@Transactional
**实现原理**
spring framework默认使用aop代理，在代码运行时生成一个代理对象，根据@Transactional的属性配置信息，决定声明@Transactional的目标方法是否由拦截器TransactionInterceptor来使用拦截，在拦截时，会在目标方法开始执行之前创建并加入事务，并执行目标方法的逻辑，最后根据执行情况是否出现异常，利用抽象事务管理器AbstractPlatformTransactionManager操作数据源DataSource提交或回滚事务

## 如果数据量过大怎么办
### 垂直切分
按照业务将表进行分类，分布到不同的数据库上面

**优点**
1. 拆分后业务清晰，拆分规则明确。
2. 系统之间整合或扩展容易。
3. 数据维护简单。

**缺点**
1. 部分业务表无法join，只能通过接口方式解决，提高了系统复杂度
2. 受每种业务不同的限制存在单库性能瓶颈，不易数据扩展和性能提高
3. 事务处理复杂
### 水平切分
把同一个表拆到不同的数据库中(按照数据行去切分)
有两种模式
- 分库
![](https://img-blog.csdn.net/20160910084749776?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
- 分表
![](https://img-blog.csdn.net/20160910084811416?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

**优点**
1. 不存在单库大数据，高并发的性能瓶颈
2. 对应用透明，应用端改造较少
3. 按照合理拆分规则拆分，join操作基本避免跨库
4. 提高了系统的稳定性和负载能力

**缺点**
1. 拆分规则难以抽象
2. 分片事务一致性难以解决
3. 数据多次扩展难度和维护量极大
4. 跨库join性能较差

### 如何管理多数据源
- 客户端模式，在每个应用程序模块中配置自己需要的数据源，直接访问各个数据库，在模块内完成数据的整合。
  - 优点是实现相对简单，无性能损耗
  - 缺点是不够通用，数据库连接的处理复杂，对业务不够透明，处理复杂

- 通过中间代理层来统一管理所有的数据源，后端数据库集群对前端应用程序透明
  - 优点：通用，对应用透明，改造少 
  - 缺点是实现难度大，有二次转发性能损失
###   分库分表带来的问题
#### 事务一致性问题
1. 分布式事务
一般使用XA协议和两阶段提交处理
2. 最终一致性
对于不可求系统的实时一致性，只要在允许的时间段内达到最终一致性即可的系统，可以采用事务补偿的方式。
事务补偿是指在事后检查补救，例如对数据进行对账检查，基于日志进行对比，定期同标准数据来源进行同步等等
#### 跨节点关联查询join问题
切分之后的join问题，对于该问题的解决方法如下:
1. 全局表
也可以看作是数据字典表，对于系统中所有模块都可能依赖的一些表，为了避免join查询，可以把这类表在每个数据库中都保存一份。这些表中的数据应该具有很少被修改的特性
2. 字段冗余
一种典型的反范式设计，利用空间换时间，为了性能避免join查询。例如订单表保存userId时，也将userName冗余保存一份，查询订单时就不用再去查user表了
3. 数据组装
在系统层面，分两次查询，第一次查询的结果集中找出关联数据id，然后根据id发起第二次请求得到关联数据。最后将这些数据进行字段拼装
4. ER分片
根据表的关联关系，将存在关联关系的表放在同一个分片上

![](https://gitee.com/zacharytse/image/raw/master/img/20201205163455.png)
#### 跨节点分页，排序、函数问题
需要先在不同的分片节点中将数据进行排序并返回，然后将不同分片返回的结果集进行汇总和再次排序，最终返回给用户
![](https://gitee.com/zacharytse/image/raw/master/img/20201205163729.png)
#### 全局主键避重问题
在分库分表环境中，由于表中数据同时存在不同数据库中，主键值平时使用的自增长将无用武之地，某个分区数据库自生成的 ID 无法保证全局唯一。常用的方案如下：
1. UUID
包含32个16进制数字，分为5段，形式为8-4-4-4-12
2. 结合数据库维护主键ID表
在数据库中建立sequence表，作为全局索引
```sql
CREATE TABLE `sequence` (   
  `id` bigint(20) unsigned NOT NULL auto_increment,   
  `stub` char(1) NOT NULL default '',   
  PRIMARY KEY  (`id`),   
  UNIQUE KEY `stub` (`stub`)   
) ENGINE=MyISAM; 
```
这里使用MyISAM是因为它使用的是表级别的锁，对表的读写是串行的，所以不用担心并发读取表的问题

sequence表的内容如下:
```
+-------------------+------+   
| id                | stub |   
+-------------------+------+   
| 72157623227190423 |    a |
+-------------------+------+
```
当需要全局64位id时，执行
```sql
REPLACE INTO sequence (stub) VALUES ('a');   
SELECT LAST_INSERT_ID();   
```
replace into是insert into的升级版，如果插入的值已经有了，则删除这一行，并插入新的一行，如果没有，则直接执行插入操作。这样就避免了表行数过大的问题，同时也不需要定期清理

但这个方案最大的问题在于单库的读写性能。所以对次的优化是在多个库上建立全局sequence表，每个表的id增长的步长是库的数量，起始值依次错开
![](https://gitee.com/zacharytse/image/raw/master/img/20201205171048.png)
但这里每次获取id仍然要读取数据库。可以采取批量的方式来降低数据库的压力，每次获取一段区间的id号段，用完后，再从数据库获取.
![](https://gitee.com/zacharytse/image/raw/master/img/20201205171243.png)
数据库中只存储当前最大ID。ID生成服务每次批量拉取6个id，先将max_id修改为5，当应用访问id生成服务时，就不要访问数据库，从缓存中获取id即可
3. Snowflake分布式自增ID算法
组成
- 第一位未使用。
- 接下来 41 位是毫秒级时间，41 位的长度可以表示 69 年的时间。
- 5 位 datacenterId，5 位 workerId。10 位的长度最多支持部署 1024 个节点。
- 最后 12 位是毫秒内的计数，12 位的计数顺序号支持每个节点每毫秒产生 4096 个 ID 序列。
![](https://gitee.com/zacharytse/image/raw/master/img/20201205171532.png)
但这样做，如果时钟回拨，则可能导致生成ID重复
## 安全性和可用性
采用垂直切分，将不相关的业务的数据库分割，不能因为一个业务影响其他业务
利用水平切分，当一个数据库出现问题，不会影响到所有用户。